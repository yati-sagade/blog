<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="dcterms.date" content="2023-04-30" />
  <title>Notes on CNNs</title>
  <style>
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      background-color: #1a1a1a;
      border: none;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
</head>
<body>
<nav id="mainnav">
  <a href="index.html">Home</a>
</nav>
<header id="title-block-header">
<h1 class="title">Notes on CNNs</h1>
<p class="date">2023-04-30</p>
</header>
<p>These are my notes about CNNs, some toy code in <a
href="https://colab.research.google.com/drive/1xz2XLOVK9NTTYGjhPcFvc5e5lDy0sgHR#scrollTo=dbM3v1sNNHOf">this
colab</a>.</p>
<h2 id="motivation-for-cnns">Motivation for CNNs</h2>
<p>While MLPs are a very general class of models that work even for
image data, they fail to take into account important properties that 2D
images have.</p>
<p>For instance, typical MLPs would take a flattened 1D vector of pixels
from the image as input, and potentially ignore the rich 2D local
information (although I wonder if with enough tuning, it could just
learn the associations, since an image is also stored on the computer as
a sequence of numbers).</p>
<p>But the more insidious problem is that of the parameter space.
Consider a reasonable input size of 100x100 images. A single fully
connected layer with 1000 units would need <span
class="math inline">\(10^7\)</span> parameters. But 1000 hidden units is
usually too less.</p>
<p>When we constrain these models by introducing good inductive biases,
we can reduce the required number of parameters drastically.</p>
<p>The main constraints we can put on 2D image data are:</p>
<ol type="1">
<li>Translation equivariance: For most object detection style tasks, the
earliest layers should not care where an object is in the image should
not matter.</li>
<li>The earliest layers of the network should focus on local regions.
The local representations can eventually be pooled into higher level
representations.</li>
</ol>
<h2 id="from-fully-connected-to-local-convolutions">From fully connected
to local convolutions</h2>
<p>Suppose we start with 2D inputs <span class="math inline">\(X \in
\mathbb{R}^{4x4}\)</span> and want to compute 2D hidden representations
<span class="math inline">\(H \in \mathbb{R}^{2x2}\)</span>.</p>
<p>In a fully connected setting, we need a 4x4 weight matrix associated
with <em>each</em> <span class="math inline">\(H_{i,j}\)</span>. We need
4 of these since <span class="math inline">\(H\)</span> is itself 2x2.
So let’s put these matrices in a 4D array (or “order 4 tensor”):</p>
<p><span class="math display">\[
H_{i,j} = U_{i,j} + \sum_k \sum_l W_{i, j, k, l} X_{k, l}
\]</span></p>
<p>Visualize <span class="math inline">\(W\)</span> a bit like this:</p>
<pre><code>W = [
  [( ), ( )],
  [( ), ( )],
]   ^-------------------\
                        (
                          [. . . .],
                          [. . . .],
                          [. . . .],
                          [. . . .]
                        )
</code></pre>
<p>Each of the four <code>( )</code> is itself a 4x4 matrix as shown at
the bottom right of the figure.</p>
<p>When we want to compute, say, <span
class="math inline">\(H_{1,0}\)</span>, we pick the corresponding matrix
from W (the one that the arrow points at in the figure), perform an
element-wise multiplication with the input <span
class="math inline">\(X\)</span>, and sum up the resulting elements, and
add up the bias <span class="math inline">\(U_{1, 0}\)</span> to produce
the result.</p>
<p>Let’s change up the notation a bit now, and set <span
class="math inline">\(a = k - i\)</span>, <span class="math inline">\(b
= l - j\)</span> to get</p>
<p><span class="math display">\[
H_{i,j} = U_{i,j} + \sum_a \sum_b W_{i, j, i + a, j + b} X_{i+a, j+b}
\]</span></p>
<p>Notice that <span class="math inline">\(a, b\)</span> can be negative
as well as positive and take on values to ensure <span
class="math inline">\(i+a\)</span> and <span
class="math inline">\(j+b\)</span> range over the entire image.</p>
<p>Setting <span class="math inline">\(V_{i,j,a,b} =
W_{i,j,i+a,j+b}\)</span>, we get</p>
<p><span class="math display">\[
H_{i,j} = U_{i,j} + \sum_a \sum_b V_{i, j, a, b} X_{i+a, j+b}
\]</span></p>
<p>Now for the hidden representation to be translation equivariant
(invariant), we must have that translating the input <span
class="math inline">\(X\)</span> also translates (leaves unchanged) the
hidden representation. This is, in general, not true since we have 4
different linear actions in our case corresponding to each <span
class="math inline">\(H_{i,j}\)</span>. Sure maybe the model can
<em>learn</em> a set of matrices that roughly achieves this behaviour,
but if we simply use <em>one</em> weight matrix (and a single bias) for
all hidden units, and say that <span class="math inline">\(V_{i, j, a,
b} = V_{a, b}\)</span>, then we get</p>
<p><span class="math display">\[
H_{i,j} = u + \sum_a \sum_b V_{a, b} X_{i+a, j+b}
\]</span></p>
<p>Notice that in the current form, since <span class="math inline">\(a,
b\)</span> range over the entire image no matter where <span
class="math inline">\(i, j\)</span> are fixed, the result activation for
each hidden unit will be the same! We will fix that by making another
simplification of assuming locality – now, <span
class="math inline">\(a, b\)</span> will only take on values in <span
class="math inline">\([-Δ, Δ]\)</span>:</p>
<p><span class="math display">\[
H_{i,j} = u + \sum_{a=-\Delta}^\Delta \sum_{b=-\Delta}^\Delta V_{a, b}
X_{i+a, j+b}
\]</span></p>
<p>Now, each <span class="math inline">\(H_{i,j}\)</span> gets a
different patch of the input image to sum over. The “kernel” or the
weight matrix stays the same.</p>
<p>So far so good, but <strong>image pixels can be colour and hence
“multichannel” in how they are represented</strong>.</p>
<p>No worries though, we simply make the shift from <span
class="math inline">\(X_{i,j} → X_{i, j, k}\)</span> and correspondingly
from <span class="math inline">\(V_{a, b} → V_{a, b, c}\)</span>.</p>
<p>The idea is to also have multichannel hidden representations, because
these also prove useful to learn multiple features from the same region.
Therefore, we must also go from <span class="math inline">\(V_{a, b, c}
→ V_{a, b, c, d}\)</span> to finally have:</p>
<p><span class="math display">\[
H_{i, j, d} = \sum_{a=-\Delta}^\Delta \sum_{b=-\Delta}^\Delta \sum_c
V_{a, b, c, d} X_{i+a, j+b, c}
\]</span></p>
<p>Assuming</p>
<ul>
<li>Δ = 2</li>
<li>c~3 input channels and d~2 hidden channels,</li>
</ul>
<pre><code>V = [
  [( ), ( )],
  [( ), ( )],
]   ^----------V[a, b]--=
                        (
                          [. .],
                          [. .],
                          [. .],
                        ){3x2}</code></pre>
<p>Now each <span class="math inline">\(V_{a, b}\)</span> is a 3x2
matrix.</p>
<h3 id="convolution-or-cross-correlation">Convolution or
cross-correlation?</h3>
<p>In deep learning, the thing we call a convolution is technically the
cross-correlation operation. This does not matter for convolutional
models trained from data though, since they’d simply learn a flipped
kernel if we use the true convolution operation.</p>
<h3 id="padding">Padding</h3>
<p>2D convolutions produce an output that is smaller than the input. For
a <code>(h, w)</code> input and a <code>(kh, kw)</code> kernel, the
output has the shape <code>(h - kh + 1, w - kw + 1)</code>.</p>
<p>In a convolutional network, successive conv operations can diminish
the size of the hidden representations drastically. e.g., if we start
with a 200x200 input and apply 10 layers of convolutions with a 5x5
kernel each time, we end up with a size of 140x140.</p>
<p>More problematic than this is perhaps how the boundary pixels are
underutilized. The 4 corner pixels are only used once, for instance.</p>
<p>A common mitigation for this is to apply padding (usually zeros) to
the image.</p>
<p>Adding a padding of <code>ph</code> pixels to the height (about half
at the top, half at the bottom) and <code>pw</code> to the width, we get
an output size of <code>(h + ph - kh + 1, w + pw - kw + 1)</code>. To
preserve the size of the input, set <code>ph = kh - 1</code> and
<code>pw = kw - 1</code>.</p>
<h3 id="strides">Strides</h3>
<p>Another technique to control the output image size involves
controlling the stride of the sliding window. By default we slide 1px at
at time, but we can jump over pixels to get a scaled down output.</p>
<p>If the stride in the height and width directions is
<code>(sh, sw)</code>, then the output is of size can be reasoned about
as follows (only showing the derivation for the width, the height has a
symmetrical derivation):</p>
<ol type="1">
<li>Without any stride or padding, we have <code>(w - kw + 1)</code>
output elements because you start with the kernel at the beginning, and
can then slide it to the right <code>(w - kw)</code> times.</li>
<li>But with a stride, one can only slide it
<code>⌊(w - kw) / sw⌋</code> times. Therefore, with a stride, we have
<code>⌊(w - kw) / sw⌋⌋+ 1</code> output elements.</li>
<li>Adding padding to the mix, assume we add symmetric padding
<code>p</code> on both sides for a total padding of <code>2p</code>
horizontally, we get <code>⌊(w - kw + 2p)⌋ / sw⌋ + 1</code> as the final
expression for the output size.</li>
</ol>
<p>Notes:</p>
<ul>
<li>When <code>2p = kw-1</code> (so <code>kw</code> must be odd and
&gt;2), the output size can be written as
<code>⌊(w - 1) / sw⌋ + 1</code> = <code>⌊(w + sw - 1) / sw⌋</code>.</li>
<li>When the width w is divisible by sw, this further simplifies to
<code>w / sw</code>.</li>
</ul>
<p>Reference on more conv arithmetic:
https://arxiv.org/pdf/1603.07285.pdf</p>
<p>Fractional strides are possible when you pad between pixels.</p>
<h2 id="training">Training</h2>
<p>I realized that cnns are incredibly finicky when it comes to things
like the SGD batch size.</p>
</body>
</html>
