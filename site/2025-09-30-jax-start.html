<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="dcterms.date" content="2025-09-30" />
  <title>Learning JAX, good ol’ MLP</title>
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
    /* CSS for syntax highlighting */
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #aaaaaa;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
    div.sourceCode
      {   }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span.al { color: #ff0000; font-weight: bold; } /* Alert */
    code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
    code span.at { color: #7d9029; } /* Attribute */
    code span.bn { color: #40a070; } /* BaseN */
    code span.bu { color: #008000; } /* BuiltIn */
    code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
    code span.ch { color: #4070a0; } /* Char */
    code span.cn { color: #880000; } /* Constant */
    code span.co { color: #60a0b0; font-style: italic; } /* Comment */
    code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
    code span.do { color: #ba2121; font-style: italic; } /* Documentation */
    code span.dt { color: #902000; } /* DataType */
    code span.dv { color: #40a070; } /* DecVal */
    code span.er { color: #ff0000; font-weight: bold; } /* Error */
    code span.ex { } /* Extension */
    code span.fl { color: #40a070; } /* Float */
    code span.fu { color: #06287e; } /* Function */
    code span.im { color: #008000; font-weight: bold; } /* Import */
    code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
    code span.kw { color: #007020; font-weight: bold; } /* Keyword */
    code span.op { color: #666666; } /* Operator */
    code span.ot { color: #007020; } /* Other */
    code span.pp { color: #bc7a00; } /* Preprocessor */
    code span.sc { color: #4070a0; } /* SpecialChar */
    code span.ss { color: #bb6688; } /* SpecialString */
    code span.st { color: #4070a0; } /* String */
    code span.va { color: #19177c; } /* Variable */
    code span.vs { color: #4070a0; } /* VerbatimString */
    code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
  </style>
  <link rel="stylesheet" href="style.css" />
  <script>
  document.addEventListener('DOMContentLoaded', function() {
    document.querySelectorAll('.zippy').forEach(zippy => {
      const previewText = zippy.getAttribute('data-preview') || 'Click to expand';
      zippy.innerHTML = `
        <div class="zippy-header-collapsed">${previewText} ▶</div>
        <div class="zippy-header-expanded">${previewText} ▼</div>
        <div class="zippy-content">${zippy.innerHTML}</div>
      `;

      zippy.addEventListener('click', function() {
        this.classList.toggle('expanded');
      });
    });
  });
  </script>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
</head>
<body>
<nav id="mainnav">
  <a href="index.html">Home</a>
</nav>
<header id="title-block-header">
<h1 class="title">Learning JAX, good ol’ MLP</h1>
<p class="date">2025-09-30</p>
</header>
<p>I want to get started with JAX <em>fast</em> and am starting with a
simple MLP with a continuous output. Rules:</p>
<ol type="1">
<li>Hand-write all code: Autocomplete OK, AI-based mindreading to be
used sparingly only for boilerplate I am familiar with.</li>
<li>Ask ChatGPT/Claude to critique the code</li>
<li>Incorporate feedback</li>
</ol>
<p>I am slightly familiar with PyTorch, so obviously I started with a
<code>class MLP</code>. As it turns out, this is not the most idiomatic
way to write JAX code, more on that later. But here’s the first version,
including all its bugs.</p>
<section class="zippy" data-preview="Click to see v1...">
<div class="sourceCode" id="cb1"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> jax</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> jax.numpy <span class="im">as</span> jnp</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Synthetic data gen</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>N <span class="op">=</span> <span class="dv">10000</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>key <span class="op">=</span> jax.random.PRNGKey(<span class="dv">42</span>)</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>key, subkey <span class="op">=</span> jax.random.split(key)</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> jax.random.normal(subkey, (N, <span class="dv">5</span>))</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> real_func(xs):</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> jnp.<span class="bu">sum</span>(xs, axis<span class="op">=</span><span class="dv">1</span>, keepdims<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>ys <span class="op">=</span> real_func(X)</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&#39;Generated synthetic data: X: </span><span class="sc">{</span>X<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">, ys: </span><span class="sc">{</span>ys<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">&#39;</span>)</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Model meta defn.</span></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> MLP:</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>  <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, key, input_size, hidden_sizes, output_size):</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>    <span class="va">self</span>.params <span class="op">=</span> []</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>    sizes <span class="op">=</span> [input_size] <span class="op">+</span> hidden_sizes</span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>    <span class="va">self</span>.nparams <span class="op">=</span> []</span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i, (in_size, out_size) <span class="kw">in</span> <span class="bu">enumerate</span>(<span class="bu">zip</span>(sizes[:<span class="op">-</span><span class="dv">1</span>], sizes[<span class="dv">1</span>:])):</span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>      key, subkey <span class="op">=</span> jax.random.split(key)</span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>      W <span class="op">=</span> jax.random.normal(subkey, shape<span class="op">=</span>(out_size, in_size)) <span class="op">*</span> jax.numpy.sqrt(<span class="fl">2.</span> <span class="op">/</span> in_size)</span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a>      b <span class="op">=</span> jax.random.normal(subkey, shape<span class="op">=</span>(out_size,)) <span class="op">*</span> jax.numpy.sqrt(<span class="fl">2.</span> <span class="op">/</span> in_size)</span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>      g <span class="op">=</span> jax.random.normal(subkey, shape<span class="op">=</span>(out_size,)) <span class="op">*</span> jax.numpy.sqrt(<span class="fl">2.</span> <span class="op">/</span> in_size) <span class="co"># Adaptive gain for LayerNorm</span></span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a>      <span class="va">self</span>.nparams.append(<span class="dv">3</span>)</span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a>      <span class="va">self</span>.params.extend((W, b, g))</span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a>    W_out <span class="op">=</span> jax.random.normal(subkey, shape<span class="op">=</span>(output_size, hidden_sizes[<span class="op">-</span><span class="dv">1</span>])) <span class="op">*</span> jax.numpy.sqrt(<span class="fl">2.</span> <span class="op">/</span> hidden_sizes[<span class="op">-</span><span class="dv">1</span>])</span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a>    b_out <span class="op">=</span> jax.random.normal(subkey, shape<span class="op">=</span>(output_size,))</span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a>    <span class="va">self</span>.params.extend((W_out, b_out))</span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a>    <span class="va">self</span>.nparams.append(<span class="dv">2</span>)</span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a>    <span class="va">self</span>._loss_and_grad <span class="op">=</span> jax.jit(jax.value_and_grad(<span class="va">self</span>._loss))</span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a>  <span class="kw">def</span> _forward(<span class="va">self</span>, params, xs):</span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, <span class="bu">len</span>(params)<span class="op">-</span><span class="dv">2</span>, <span class="dv">3</span>):</span>
<span id="cb1-43"><a href="#cb1-43" aria-hidden="true" tabindex="-1"></a>      W, b, g <span class="op">=</span> params[i:i<span class="op">+</span><span class="dv">3</span>]</span>
<span id="cb1-44"><a href="#cb1-44" aria-hidden="true" tabindex="-1"></a>      <span class="co"># LayerNorm</span></span>
<span id="cb1-45"><a href="#cb1-45" aria-hidden="true" tabindex="-1"></a>      a <span class="op">=</span> xs <span class="op">@</span> W.T</span>
<span id="cb1-46"><a href="#cb1-46" aria-hidden="true" tabindex="-1"></a>      mu <span class="op">=</span> a.mean(axis<span class="op">=</span><span class="dv">1</span>, keepdims<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb1-47"><a href="#cb1-47" aria-hidden="true" tabindex="-1"></a>      sigma <span class="op">=</span> a.std(axis<span class="op">=</span><span class="dv">1</span>, keepdims<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb1-48"><a href="#cb1-48" aria-hidden="true" tabindex="-1"></a>      <span class="co"># print(f&#39;mu:{mu.item()}:{mu.shape}, sigma:{sigma.item()}:{sigma.shape}&#39;)</span></span>
<span id="cb1-49"><a href="#cb1-49" aria-hidden="true" tabindex="-1"></a>      a <span class="op">=</span> g <span class="op">*</span> (a <span class="op">-</span> mu) <span class="op">/</span> sigma</span>
<span id="cb1-50"><a href="#cb1-50" aria-hidden="true" tabindex="-1"></a>      <span class="co"># print(f&#39;a:{a.shape}&#39;)</span></span>
<span id="cb1-51"><a href="#cb1-51" aria-hidden="true" tabindex="-1"></a>      <span class="co"># ReLU</span></span>
<span id="cb1-52"><a href="#cb1-52" aria-hidden="true" tabindex="-1"></a>      xs <span class="op">=</span> jax.nn.relu(a <span class="op">+</span> b)</span>
<span id="cb1-53"><a href="#cb1-53" aria-hidden="true" tabindex="-1"></a>    W, b <span class="op">=</span> params[<span class="op">-</span><span class="dv">2</span>:]</span>
<span id="cb1-54"><a href="#cb1-54" aria-hidden="true" tabindex="-1"></a>    xs <span class="op">=</span> xs <span class="op">@</span> W.T <span class="op">+</span> b</span>
<span id="cb1-55"><a href="#cb1-55" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> xs</span>
<span id="cb1-56"><a href="#cb1-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-57"><a href="#cb1-57" aria-hidden="true" tabindex="-1"></a>  <span class="kw">def</span> forward(<span class="va">self</span>, xs):</span>
<span id="cb1-58"><a href="#cb1-58" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="va">self</span>._forward(<span class="va">self</span>.params, xs)</span>
<span id="cb1-59"><a href="#cb1-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-60"><a href="#cb1-60" aria-hidden="true" tabindex="-1"></a>  <span class="kw">def</span> _loss(<span class="va">self</span>, params, xs, ys):</span>
<span id="cb1-61"><a href="#cb1-61" aria-hidden="true" tabindex="-1"></a>    preds <span class="op">=</span> <span class="va">self</span>._forward(params, xs)</span>
<span id="cb1-62"><a href="#cb1-62" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> preds.shape <span class="op">==</span> ys.shape, <span class="ss">f&#39;prediction shape </span><span class="sc">{</span>preds<span class="sc">.</span>shape<span class="sc">}</span><span class="ss"> != truth shape </span><span class="sc">{</span>ys<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">&#39;</span></span>
<span id="cb1-63"><a href="#cb1-63" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> jax.numpy.mean((ys <span class="op">-</span> preds) <span class="op">**</span> <span class="dv">2</span>)</span>
<span id="cb1-64"><a href="#cb1-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-65"><a href="#cb1-65" aria-hidden="true" tabindex="-1"></a>  <span class="kw">def</span> loss(<span class="va">self</span>, xs, ys):</span>
<span id="cb1-66"><a href="#cb1-66" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="va">self</span>._loss(<span class="va">self</span>.params, xs, ys)</span>
<span id="cb1-67"><a href="#cb1-67" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-68"><a href="#cb1-68" aria-hidden="true" tabindex="-1"></a>  <span class="kw">def</span> loss_and_grad(<span class="va">self</span>, xs, ys):</span>
<span id="cb1-69"><a href="#cb1-69" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="va">self</span>._loss_and_grad(<span class="va">self</span>.params, xs, ys)</span>
<span id="cb1-70"><a href="#cb1-70" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-71"><a href="#cb1-71" aria-hidden="true" tabindex="-1"></a><span class="co"># Train/val split</span></span>
<span id="cb1-72"><a href="#cb1-72" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-73"><a href="#cb1-73" aria-hidden="true" tabindex="-1"></a>ntrain <span class="op">=</span> <span class="bu">int</span>(<span class="fl">.8</span> <span class="op">*</span> N)</span>
<span id="cb1-74"><a href="#cb1-74" aria-hidden="true" tabindex="-1"></a>xs_train, ys_train <span class="op">=</span> X[:ntrain], ys[:ntrain]</span>
<span id="cb1-75"><a href="#cb1-75" aria-hidden="true" tabindex="-1"></a>xs_val, ys_val <span class="op">=</span> X[ntrain:], ys[ntrain:]</span>
<span id="cb1-76"><a href="#cb1-76" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&#39;Training set: </span><span class="sc">{</span>xs_train<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">, </span><span class="sc">{</span>ys_train<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">&#39;</span>)</span>
<span id="cb1-77"><a href="#cb1-77" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&#39;Validation set: </span><span class="sc">{</span>xs_val<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">, </span><span class="sc">{</span>ys_val<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">&#39;</span>)</span>
<span id="cb1-78"><a href="#cb1-78" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-79"><a href="#cb1-79" aria-hidden="true" tabindex="-1"></a><span class="co"># Model instantiation</span></span>
<span id="cb1-80"><a href="#cb1-80" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-81"><a href="#cb1-81" aria-hidden="true" tabindex="-1"></a>key, subkey <span class="op">=</span> jax.random.split(key)</span>
<span id="cb1-82"><a href="#cb1-82" aria-hidden="true" tabindex="-1"></a>mlp <span class="op">=</span> MLP(subkey, <span class="dv">5</span>, [<span class="dv">8</span>, <span class="dv">6</span>, <span class="dv">6</span>], <span class="dv">1</span>)</span>
<span id="cb1-83"><a href="#cb1-83" aria-hidden="true" tabindex="-1"></a><span class="co"># print(mlp.loss_and_grad(xs_train[:1], ys_train[:1]))</span></span>
<span id="cb1-84"><a href="#cb1-84" aria-hidden="true" tabindex="-1"></a><span class="co"># print(&#39;Weights\n&#39;, mlp.params)</span></span>
<span id="cb1-85"><a href="#cb1-85" aria-hidden="true" tabindex="-1"></a><span class="co"># print(&#39;Weight shapes\n&#39;, [p.shape for p in mlp.params])</span></span>
<span id="cb1-86"><a href="#cb1-86" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-87"><a href="#cb1-87" aria-hidden="true" tabindex="-1"></a><span class="co"># Training</span></span>
<span id="cb1-88"><a href="#cb1-88" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-89"><a href="#cb1-89" aria-hidden="true" tabindex="-1"></a>lr <span class="op">=</span> <span class="fl">0.01</span></span>
<span id="cb1-90"><a href="#cb1-90" aria-hidden="true" tabindex="-1"></a>niter <span class="op">=</span> <span class="dv">1000</span></span>
<span id="cb1-91"><a href="#cb1-91" aria-hidden="true" tabindex="-1"></a>nbatch <span class="op">=</span> <span class="dv">256</span></span>
<span id="cb1-92"><a href="#cb1-92" aria-hidden="true" tabindex="-1"></a>idx <span class="op">=</span> jax.random.permutation(subkey, ntrain)</span>
<span id="cb1-93"><a href="#cb1-93" aria-hidden="true" tabindex="-1"></a>start <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb1-94"><a href="#cb1-94" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(niter):</span>
<span id="cb1-95"><a href="#cb1-95" aria-hidden="true" tabindex="-1"></a>  key, subkey <span class="op">=</span> jax.random.split(key)</span>
<span id="cb1-96"><a href="#cb1-96" aria-hidden="true" tabindex="-1"></a>  idx <span class="op">=</span> jax.random.permutation(subkey, ntrain)[:nbatch]</span>
<span id="cb1-97"><a href="#cb1-97" aria-hidden="true" tabindex="-1"></a>  xb, yb <span class="op">=</span> xs_train[idx[start:start<span class="op">+</span>nbatch]], ys_train[idx[start:start<span class="op">+</span>nbatch]]</span>
<span id="cb1-98"><a href="#cb1-98" aria-hidden="true" tabindex="-1"></a>  start <span class="op">+=</span> nbatch</span>
<span id="cb1-99"><a href="#cb1-99" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> start <span class="op">&gt;=</span> ntrain:</span>
<span id="cb1-100"><a href="#cb1-100" aria-hidden="true" tabindex="-1"></a>    start <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb1-101"><a href="#cb1-101" aria-hidden="true" tabindex="-1"></a>    key, subkey <span class="op">=</span> jax.random.split(key)</span>
<span id="cb1-102"><a href="#cb1-102" aria-hidden="true" tabindex="-1"></a>    idx <span class="op">=</span> jax.random.permutation(subkey, ntrain)</span>
<span id="cb1-103"><a href="#cb1-103" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-104"><a href="#cb1-104" aria-hidden="true" tabindex="-1"></a>  l, dl <span class="op">=</span> mlp.loss_and_grad(xb, yb)</span>
<span id="cb1-105"><a href="#cb1-105" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> i <span class="op">%</span> <span class="dv">100</span> <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb1-106"><a href="#cb1-106" aria-hidden="true" tabindex="-1"></a>    train_loss <span class="op">=</span> mlp.loss(xs_train, ys_train)</span>
<span id="cb1-107"><a href="#cb1-107" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f&#39;Iteration </span><span class="sc">{</span>i<span class="sc">}</span><span class="ss">, batch loss = </span><span class="sc">{</span>l<span class="sc">.</span>item()<span class="sc">}</span><span class="ss">, train loss = </span><span class="sc">{</span>train_loss<span class="sc">.</span>item()<span class="sc">}</span><span class="ss">&#39;</span>)</span>
<span id="cb1-108"><a href="#cb1-108" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> k <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(mlp.params)):</span>
<span id="cb1-109"><a href="#cb1-109" aria-hidden="true" tabindex="-1"></a>    mlp.params[k] <span class="op">-=</span> lr <span class="op">*</span> dl[k]</span>
<span id="cb1-110"><a href="#cb1-110" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-111"><a href="#cb1-111" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&#39;Final train loss = </span><span class="sc">{</span>mlp<span class="sc">.</span>loss(xs_train, ys_train)<span class="sc">.</span>item()<span class="sc">}</span><span class="ss">&#39;</span>)</span>
<span id="cb1-112"><a href="#cb1-112" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&#39;Final val loss = </span><span class="sc">{</span>mlp<span class="sc">.</span>loss(xs_val, ys_val)<span class="sc">.</span>item()<span class="sc">}</span><span class="ss">&#39;</span>)</span></code></pre></div>
<pre><code>Generated synthetic data: X: (10000, 5), ys: (10000, 1)
Training set: (8000, 5), (8000, 1)
Validation set: (2000, 5), (2000, 1)
Iteration 0, batch loss = 13.008309364318848, train loss = 13.735170364379883
Iteration 100, batch loss = nan, train loss = 8.126752853393555
Iteration 200, batch loss = nan, train loss = 7.5402913093566895
Iteration 300, batch loss = nan, train loss = 7.13360595703125
Iteration 400, batch loss = nan, train loss = 6.8245697021484375
Iteration 500, batch loss = nan, train loss = 6.5315937995910645
Iteration 600, batch loss = nan, train loss = 6.269874572753906
Iteration 700, batch loss = nan, train loss = 6.0824151039123535
Iteration 800, batch loss = 6.443068504333496, train loss = 5.874807834625244
Iteration 900, batch loss = nan, train loss = 5.619460105895996
Final train loss = 5.504083156585693
Final val loss = 5.637689113616943</code></pre>
</section>
<p>Both the final training and validation losses are high for a simple
function and a frankly large network. I stared the code down, but I’ll
be honest, I couldn’t figure what’s going on. So I moved on to step two,
ask for a critique.</p>
<p>A summary of fixes:</p>
<p><strong>1. Use separate RNG keys for weight initialization</strong>:
The following reuses the random state and can cause the parameters to be
correlated, which is no good. When I made this change alone.</p>
<pre><code>      key, subkey = jax.random.split(key)
      W = jax.random.normal(subkey, shape=(out_size, in_size)) * jax.numpy.sqrt(2. / in_size)
      b = jax.random.normal(subkey, shape=(out_size,)) * jax.numpy.sqrt(2. / in_size)
      g = jax.random.normal(subkey, shape=(out_size,)) * jax.numpy.sqrt(2. / in_size) # Adaptive gain for LayerNorm</code></pre>
<p>When I made these changes alone, there wasn’t an appreciable decrease
in training or val losses. But this was a good fix.</p>
<p><strong>2. LayerNorm implementation</strong>: One was an obvious fix:
Use a small epsilon (say <span class="math inline">\(10^{-5}\)</span>)
in the denominator to guard against division by zero, in case the std is
zero. The second fix was more interesting: Apparently for LayerNorm, the
beta and gamma parameters should <em>not</em> be He initialized, but are
to be initialized to zeros and ones respectively. <strong>These fixes
moved my val loss from 5.64 to 2.86!!</strong> In hindsight, I the gain
parameters <code>g</code> starting out as all 1s makes sense – why
decimate the activations a priori by using <code>g</code> values drawn
from a normal?</p>
<p><strong>3. Batching bug</strong>: Uh, embarrasing. See how I
re-compute a 256-long <code>idx</code> array in every iteration of the
training loop, but still honest-to-god advance the <code>start</code>
index? Well, since <code>len(idx)=256</code>, the second and subsequent
iterations will attempt the indexing <code>idx[256:512]</code>,
<code>idx[512:768]</code>… which will all be empty slices… until
<code>start</code> is reset, and then we get one nonempty batch again.
Yuck. The fix was simple of course, and got us from <code>5.64</code> in
val loss to <code>0.82</code>!!</p>
<p><strong>4. JIT’ing the core</strong>: I am JIT’ing only the loss/grad
computation function, but the parameter updates still happen in Python
in the training loop. The idiom here is to JIT the toplevel unit that
gets invoked over and over. In our case, we’d define a
<code>train_step</code> function and move the param updates in there.
Making this change <strong>made the code much, much faster</strong>. I
did not <code>%timeit</code>, maybe I should, but the difference is
apparent.</p>
<p><strong>5. Use optax</strong>: This library provides implementations
of optimizers like AdamW, which do more sophisticated things like
momentum, gradient clipping for numerical stablity. It also becomes
easier to support learning rate schedules later. Using optax moved the
val loss from <code>0.82</code> to <code>0.57</code>. Nice, free win.
But beware, naively updating parameters in the step function won’t work.
Since JAX arrays are immutable, we have to return the updated params and
continue in our training loop with these updated params. This makes the
code awkward, but I persist in using an <code>MLP</code> class for now,
and punt the translation to a more functional style to the next
iteration.</p>
<section class="zippy" data-preview="Click to see v2...">
<div class="sourceCode" id="cb4"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> jax, optax</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> jax.numpy <span class="im">as</span> jnp</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Synthetic data gen</span></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>N <span class="op">=</span> <span class="dv">10000</span></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>key <span class="op">=</span> jax.random.PRNGKey(<span class="dv">42</span>)</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>key, subkey <span class="op">=</span> jax.random.split(key)</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> jax.random.normal(subkey, (N, <span class="dv">5</span>))</span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> real_func(xs):</span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> jnp.<span class="bu">sum</span>(xs, axis<span class="op">=</span><span class="dv">1</span>, keepdims<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a>ys <span class="op">=</span> real_func(X)</span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&#39;Generated synthetic data: X: </span><span class="sc">{</span>X<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">, ys: </span><span class="sc">{</span>ys<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">&#39;</span>)</span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Model meta defn.</span></span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> MLP:</span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a>  <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, key, input_size, hidden_sizes, output_size):</span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a>    <span class="va">self</span>.params <span class="op">=</span> []</span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a>    sizes <span class="op">=</span> [input_size] <span class="op">+</span> hidden_sizes</span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a>    <span class="va">self</span>.nparams <span class="op">=</span> []</span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i, (in_size, out_size) <span class="kw">in</span> <span class="bu">enumerate</span>(<span class="bu">zip</span>(sizes[:<span class="op">-</span><span class="dv">1</span>], sizes[<span class="dv">1</span>:])):</span>
<span id="cb4-26"><a href="#cb4-26" aria-hidden="true" tabindex="-1"></a>      key, wkey <span class="op">=</span> jax.random.split(key, <span class="dv">2</span>)</span>
<span id="cb4-27"><a href="#cb4-27" aria-hidden="true" tabindex="-1"></a>      W <span class="op">=</span> jax.random.normal(wkey, shape<span class="op">=</span>(out_size, in_size)) <span class="op">*</span> jax.numpy.sqrt(<span class="fl">2.</span> <span class="op">/</span> in_size)</span>
<span id="cb4-28"><a href="#cb4-28" aria-hidden="true" tabindex="-1"></a>      b <span class="op">=</span> jnp.zeros(shape<span class="op">=</span>(out_size,))</span>
<span id="cb4-29"><a href="#cb4-29" aria-hidden="true" tabindex="-1"></a>      g <span class="op">=</span> jnp.ones(shape<span class="op">=</span>(out_size,)) <span class="co"># Adaptive gain for LayerNorm</span></span>
<span id="cb4-30"><a href="#cb4-30" aria-hidden="true" tabindex="-1"></a>      <span class="va">self</span>.nparams.append(<span class="dv">3</span>)</span>
<span id="cb4-31"><a href="#cb4-31" aria-hidden="true" tabindex="-1"></a>      <span class="va">self</span>.params.extend((W, b, g))</span>
<span id="cb4-32"><a href="#cb4-32" aria-hidden="true" tabindex="-1"></a>    key, wkey, bkey <span class="op">=</span> jax.random.split(key, <span class="dv">3</span>)</span>
<span id="cb4-33"><a href="#cb4-33" aria-hidden="true" tabindex="-1"></a>    W_out <span class="op">=</span> jax.random.normal(wkey, shape<span class="op">=</span>(output_size, hidden_sizes[<span class="op">-</span><span class="dv">1</span>])) <span class="op">*</span> jax.numpy.sqrt(<span class="fl">2.</span> <span class="op">/</span> hidden_sizes[<span class="op">-</span><span class="dv">1</span>])</span>
<span id="cb4-34"><a href="#cb4-34" aria-hidden="true" tabindex="-1"></a>    b_out <span class="op">=</span> jax.random.normal(bkey, shape<span class="op">=</span>(output_size,))</span>
<span id="cb4-35"><a href="#cb4-35" aria-hidden="true" tabindex="-1"></a>    <span class="va">self</span>.params.extend((W_out, b_out))</span>
<span id="cb4-36"><a href="#cb4-36" aria-hidden="true" tabindex="-1"></a>    <span class="va">self</span>.nparams.append(<span class="dv">2</span>)</span>
<span id="cb4-37"><a href="#cb4-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-38"><a href="#cb4-38" aria-hidden="true" tabindex="-1"></a>    <span class="va">self</span>._loss_and_grad <span class="op">=</span> jax.value_and_grad(<span class="va">self</span>._loss)</span>
<span id="cb4-39"><a href="#cb4-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-40"><a href="#cb4-40" aria-hidden="true" tabindex="-1"></a>  <span class="kw">def</span> _forward(<span class="va">self</span>, params, xs):</span>
<span id="cb4-41"><a href="#cb4-41" aria-hidden="true" tabindex="-1"></a>    eps <span class="op">=</span> <span class="fl">1e-5</span></span>
<span id="cb4-42"><a href="#cb4-42" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, <span class="bu">len</span>(params)<span class="op">-</span><span class="dv">2</span>, <span class="dv">3</span>):</span>
<span id="cb4-43"><a href="#cb4-43" aria-hidden="true" tabindex="-1"></a>      W, b, g <span class="op">=</span> params[i:i<span class="op">+</span><span class="dv">3</span>]</span>
<span id="cb4-44"><a href="#cb4-44" aria-hidden="true" tabindex="-1"></a>      <span class="co"># LayerNorm</span></span>
<span id="cb4-45"><a href="#cb4-45" aria-hidden="true" tabindex="-1"></a>      a <span class="op">=</span> xs <span class="op">@</span> W.T</span>
<span id="cb4-46"><a href="#cb4-46" aria-hidden="true" tabindex="-1"></a>      mu <span class="op">=</span> a.mean(axis<span class="op">=</span><span class="dv">1</span>, keepdims<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb4-47"><a href="#cb4-47" aria-hidden="true" tabindex="-1"></a>      sigma <span class="op">=</span> a.std(axis<span class="op">=</span><span class="dv">1</span>, keepdims<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb4-48"><a href="#cb4-48" aria-hidden="true" tabindex="-1"></a>      <span class="co"># print(f&#39;mu:{mu.item()}:{mu.shape}, sigma:{sigma.item()}:{sigma.shape}&#39;)</span></span>
<span id="cb4-49"><a href="#cb4-49" aria-hidden="true" tabindex="-1"></a>      a <span class="op">=</span> g <span class="op">*</span> (a <span class="op">-</span> mu) <span class="op">/</span> (eps <span class="op">+</span> sigma)</span>
<span id="cb4-50"><a href="#cb4-50" aria-hidden="true" tabindex="-1"></a>      <span class="co"># print(f&#39;a:{a.shape}&#39;)</span></span>
<span id="cb4-51"><a href="#cb4-51" aria-hidden="true" tabindex="-1"></a>      <span class="co"># ReLU</span></span>
<span id="cb4-52"><a href="#cb4-52" aria-hidden="true" tabindex="-1"></a>      xs <span class="op">=</span> jax.nn.relu(a <span class="op">+</span> b)</span>
<span id="cb4-53"><a href="#cb4-53" aria-hidden="true" tabindex="-1"></a>    W, b <span class="op">=</span> params[<span class="op">-</span><span class="dv">2</span>:]</span>
<span id="cb4-54"><a href="#cb4-54" aria-hidden="true" tabindex="-1"></a>    xs <span class="op">=</span> xs <span class="op">@</span> W.T <span class="op">+</span> b</span>
<span id="cb4-55"><a href="#cb4-55" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> xs</span>
<span id="cb4-56"><a href="#cb4-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-57"><a href="#cb4-57" aria-hidden="true" tabindex="-1"></a>  <span class="kw">def</span> forward(<span class="va">self</span>, xs):</span>
<span id="cb4-58"><a href="#cb4-58" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="va">self</span>._forward(<span class="va">self</span>.params, xs)</span>
<span id="cb4-59"><a href="#cb4-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-60"><a href="#cb4-60" aria-hidden="true" tabindex="-1"></a>  <span class="kw">def</span> _loss(<span class="va">self</span>, params, xs, ys):</span>
<span id="cb4-61"><a href="#cb4-61" aria-hidden="true" tabindex="-1"></a>    preds <span class="op">=</span> <span class="va">self</span>._forward(params, xs)</span>
<span id="cb4-62"><a href="#cb4-62" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> preds.shape <span class="op">==</span> ys.shape, <span class="ss">f&#39;prediction shape </span><span class="sc">{</span>preds<span class="sc">.</span>shape<span class="sc">}</span><span class="ss"> != truth shape </span><span class="sc">{</span>ys<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">&#39;</span></span>
<span id="cb4-63"><a href="#cb4-63" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> jax.numpy.mean((ys <span class="op">-</span> preds) <span class="op">**</span> <span class="dv">2</span>)</span>
<span id="cb4-64"><a href="#cb4-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-65"><a href="#cb4-65" aria-hidden="true" tabindex="-1"></a>  <span class="kw">def</span> loss(<span class="va">self</span>, xs, ys):</span>
<span id="cb4-66"><a href="#cb4-66" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="va">self</span>._loss(<span class="va">self</span>.params, xs, ys)</span>
<span id="cb4-67"><a href="#cb4-67" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-68"><a href="#cb4-68" aria-hidden="true" tabindex="-1"></a>  <span class="kw">def</span> loss_and_grad(<span class="va">self</span>, xs, ys):</span>
<span id="cb4-69"><a href="#cb4-69" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="va">self</span>._loss_and_grad(<span class="va">self</span>.params, xs, ys)</span>
<span id="cb4-70"><a href="#cb4-70" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-71"><a href="#cb4-71" aria-hidden="true" tabindex="-1"></a><span class="co"># Train/val split</span></span>
<span id="cb4-72"><a href="#cb4-72" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-73"><a href="#cb4-73" aria-hidden="true" tabindex="-1"></a>ntrain <span class="op">=</span> <span class="bu">int</span>(<span class="fl">.8</span> <span class="op">*</span> N)</span>
<span id="cb4-74"><a href="#cb4-74" aria-hidden="true" tabindex="-1"></a>xs_train, ys_train <span class="op">=</span> X[:ntrain], ys[:ntrain]</span>
<span id="cb4-75"><a href="#cb4-75" aria-hidden="true" tabindex="-1"></a>xs_val, ys_val <span class="op">=</span> X[ntrain:], ys[ntrain:]</span>
<span id="cb4-76"><a href="#cb4-76" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&#39;Training set: </span><span class="sc">{</span>xs_train<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">, </span><span class="sc">{</span>ys_train<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">&#39;</span>)</span>
<span id="cb4-77"><a href="#cb4-77" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&#39;Validation set: </span><span class="sc">{</span>xs_val<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">, </span><span class="sc">{</span>ys_val<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">&#39;</span>)</span>
<span id="cb4-78"><a href="#cb4-78" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-79"><a href="#cb4-79" aria-hidden="true" tabindex="-1"></a><span class="co"># Model instantiation</span></span>
<span id="cb4-80"><a href="#cb4-80" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-81"><a href="#cb4-81" aria-hidden="true" tabindex="-1"></a>key, subkey <span class="op">=</span> jax.random.split(key)</span>
<span id="cb4-82"><a href="#cb4-82" aria-hidden="true" tabindex="-1"></a>mlp <span class="op">=</span> MLP(subkey, <span class="dv">5</span>, [<span class="dv">8</span>, <span class="dv">6</span>, <span class="dv">6</span>], <span class="dv">1</span>)</span>
<span id="cb4-83"><a href="#cb4-83" aria-hidden="true" tabindex="-1"></a><span class="co"># print(mlp.loss_and_grad(xs_train[:1], ys_train[:1]))</span></span>
<span id="cb4-84"><a href="#cb4-84" aria-hidden="true" tabindex="-1"></a><span class="co"># print(&#39;Weights\n&#39;, mlp.params)</span></span>
<span id="cb4-85"><a href="#cb4-85" aria-hidden="true" tabindex="-1"></a><span class="co"># print(&#39;Weight shapes\n&#39;, [p.shape for p in mlp.params])</span></span>
<span id="cb4-86"><a href="#cb4-86" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-87"><a href="#cb4-87" aria-hidden="true" tabindex="-1"></a><span class="co"># Training</span></span>
<span id="cb4-88"><a href="#cb4-88" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-89"><a href="#cb4-89" aria-hidden="true" tabindex="-1"></a>opt <span class="op">=</span> optax.adamw(learning_rate<span class="op">=</span><span class="fl">0.01</span>, weight_decay<span class="op">=</span><span class="fl">0.001</span>)</span>
<span id="cb4-90"><a href="#cb4-90" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-91"><a href="#cb4-91" aria-hidden="true" tabindex="-1"></a><span class="at">@jax.jit</span></span>
<span id="cb4-92"><a href="#cb4-92" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> train_step(params, opt_state, xs, ys):</span>
<span id="cb4-93"><a href="#cb4-93" aria-hidden="true" tabindex="-1"></a>  l, dl <span class="op">=</span> mlp._loss_and_grad(params, xs, ys)</span>
<span id="cb4-94"><a href="#cb4-94" aria-hidden="true" tabindex="-1"></a>  updates, opt_state <span class="op">=</span> opt.update(dl, opt_state, params)</span>
<span id="cb4-95"><a href="#cb4-95" aria-hidden="true" tabindex="-1"></a>  params <span class="op">=</span> optax.apply_updates(params, updates)</span>
<span id="cb4-96"><a href="#cb4-96" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> params, opt_state</span>
<span id="cb4-97"><a href="#cb4-97" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-98"><a href="#cb4-98" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-99"><a href="#cb4-99" aria-hidden="true" tabindex="-1"></a>key, subkey <span class="op">=</span> jax.random.split(key)</span>
<span id="cb4-100"><a href="#cb4-100" aria-hidden="true" tabindex="-1"></a>idx <span class="op">=</span> jax.random.permutation(subkey, ntrain)</span>
<span id="cb4-101"><a href="#cb4-101" aria-hidden="true" tabindex="-1"></a>niter <span class="op">=</span> <span class="dv">1000</span></span>
<span id="cb4-102"><a href="#cb4-102" aria-hidden="true" tabindex="-1"></a>nbatch <span class="op">=</span> <span class="dv">256</span></span>
<span id="cb4-103"><a href="#cb4-103" aria-hidden="true" tabindex="-1"></a>start <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb4-104"><a href="#cb4-104" aria-hidden="true" tabindex="-1"></a>opt_state <span class="op">=</span> opt.init(mlp.params)</span>
<span id="cb4-105"><a href="#cb4-105" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-106"><a href="#cb4-106" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(niter):</span>
<span id="cb4-107"><a href="#cb4-107" aria-hidden="true" tabindex="-1"></a>  key, subkey <span class="op">=</span> jax.random.split(key)</span>
<span id="cb4-108"><a href="#cb4-108" aria-hidden="true" tabindex="-1"></a>  xb, yb <span class="op">=</span> xs_train[idx[start:start<span class="op">+</span>nbatch]], ys_train[idx[start:start<span class="op">+</span>nbatch]]</span>
<span id="cb4-109"><a href="#cb4-109" aria-hidden="true" tabindex="-1"></a>  start <span class="op">+=</span> nbatch</span>
<span id="cb4-110"><a href="#cb4-110" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> start <span class="op">&gt;=</span> ntrain:</span>
<span id="cb4-111"><a href="#cb4-111" aria-hidden="true" tabindex="-1"></a>    start <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb4-112"><a href="#cb4-112" aria-hidden="true" tabindex="-1"></a>    key, subkey <span class="op">=</span> jax.random.split(key)</span>
<span id="cb4-113"><a href="#cb4-113" aria-hidden="true" tabindex="-1"></a>    idx <span class="op">=</span> jax.random.permutation(subkey, ntrain)</span>
<span id="cb4-114"><a href="#cb4-114" aria-hidden="true" tabindex="-1"></a>  mlp.params, opt_state <span class="op">=</span> train_step(mlp.params, opt_state, xb, yb)</span>
<span id="cb4-115"><a href="#cb4-115" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> i <span class="op">%</span> <span class="dv">100</span> <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb4-116"><a href="#cb4-116" aria-hidden="true" tabindex="-1"></a>    train_loss <span class="op">=</span> mlp.loss(xs_train, ys_train)</span>
<span id="cb4-117"><a href="#cb4-117" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f&#39;Iteration </span><span class="sc">{</span>i<span class="sc">}</span><span class="ss">, batch loss = </span><span class="sc">{</span>l<span class="sc">.</span>item()<span class="sc">}</span><span class="ss">, train loss = </span><span class="sc">{</span>train_loss<span class="sc">.</span>item()<span class="sc">}</span><span class="ss">&#39;</span>)</span>
<span id="cb4-118"><a href="#cb4-118" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&#39;Final train loss = </span><span class="sc">{</span>mlp<span class="sc">.</span>loss(xs_train, ys_train)<span class="sc">.</span>item()<span class="sc">}</span><span class="ss">&#39;</span>)</span>
<span id="cb4-119"><a href="#cb4-119" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&#39;Final val loss = </span><span class="sc">{</span>mlp<span class="sc">.</span>loss(xs_val, ys_val)<span class="sc">.</span>item()<span class="sc">}</span><span class="ss">&#39;</span>)</span></code></pre></div>
<pre><code>Generated synthetic data: X: (10000, 5), ys: (10000, 1)
Training set: (8000, 5), (8000, 1)
Validation set: (2000, 5), (2000, 1)
Iteration 0, batch loss = nan, train loss = 4.737359523773193
Iteration 100, batch loss = nan, train loss = 0.7498972415924072
Iteration 200, batch loss = nan, train loss = 0.6198509335517883
Iteration 300, batch loss = nan, train loss = 0.5501429438591003
Iteration 400, batch loss = nan, train loss = 0.49914073944091797
Iteration 500, batch loss = nan, train loss = 0.5024610757827759
Iteration 600, batch loss = nan, train loss = 0.48184746503829956
Iteration 700, batch loss = nan, train loss = 0.47913187742233276
Iteration 800, batch loss = nan, train loss = 0.4928268492221832
Iteration 900, batch loss = nan, train loss = 0.4822283685207367
Final train loss = 0.5036829710006714
Final val loss = 0.5718706250190735</code></pre>
</section>
<h2 id="more-idiomatic-functional-code">More idiomatic, functional
code</h2>
<p>The above code is modeled after how we’d code a model in PyTorch. And
it smells, as we have to do the weird param shuffling in and out of the
model instance. So here is the above code minus redundant parts, with
pure functions and a toplevel training loop that plumbs all required
state explicitly.</p>
<section class="zippy" data-preview="Click to see the final version...">
<div class="sourceCode" id="cb6"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> jax, optax</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> jax.numpy <span class="im">as</span> jnp</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Synthetic data gen</span></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>N <span class="op">=</span> <span class="dv">10000</span></span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>key <span class="op">=</span> jax.random.PRNGKey(<span class="dv">42</span>)</span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>key, subkey <span class="op">=</span> jax.random.split(key)</span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> jax.random.normal(subkey, (N, <span class="dv">5</span>))</span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> real_func(xs):</span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> jnp.<span class="bu">sum</span>(xs, axis<span class="op">=</span><span class="dv">1</span>, keepdims<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a>ys <span class="op">=</span> real_func(X)</span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&#39;Generated synthetic data: X: </span><span class="sc">{</span>X<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">, ys: </span><span class="sc">{</span>ys<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">&#39;</span>)</span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-18"><a href="#cb6-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Model meta defn.</span></span>
<span id="cb6-19"><a href="#cb6-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-20"><a href="#cb6-20" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> init_mlp(key, input_size, hidden_sizes, output_size):</span>
<span id="cb6-21"><a href="#cb6-21" aria-hidden="true" tabindex="-1"></a>  params <span class="op">=</span> []</span>
<span id="cb6-22"><a href="#cb6-22" aria-hidden="true" tabindex="-1"></a>  sizes <span class="op">=</span> [input_size] <span class="op">+</span> hidden_sizes</span>
<span id="cb6-23"><a href="#cb6-23" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> i, (in_size, out_size) <span class="kw">in</span> <span class="bu">enumerate</span>(<span class="bu">zip</span>(sizes[:<span class="op">-</span><span class="dv">1</span>], sizes[<span class="dv">1</span>:])):</span>
<span id="cb6-24"><a href="#cb6-24" aria-hidden="true" tabindex="-1"></a>    key, wkey <span class="op">=</span> jax.random.split(key, <span class="dv">2</span>)</span>
<span id="cb6-25"><a href="#cb6-25" aria-hidden="true" tabindex="-1"></a>    params.append(<span class="bu">dict</span>(</span>
<span id="cb6-26"><a href="#cb6-26" aria-hidden="true" tabindex="-1"></a>      W<span class="op">=</span>jax.random.normal(wkey, shape<span class="op">=</span>(out_size, in_size)) <span class="op">*</span> jax.numpy.sqrt(<span class="fl">2.</span> <span class="op">/</span> in_size),</span>
<span id="cb6-27"><a href="#cb6-27" aria-hidden="true" tabindex="-1"></a>      b<span class="op">=</span>jnp.zeros(shape<span class="op">=</span>(out_size,)),</span>
<span id="cb6-28"><a href="#cb6-28" aria-hidden="true" tabindex="-1"></a>      g<span class="op">=</span>jnp.ones(shape<span class="op">=</span>(out_size,))))</span>
<span id="cb6-29"><a href="#cb6-29" aria-hidden="true" tabindex="-1"></a>  key, wkey, bkey <span class="op">=</span> jax.random.split(key, <span class="dv">3</span>)</span>
<span id="cb6-30"><a href="#cb6-30" aria-hidden="true" tabindex="-1"></a>  W_out <span class="op">=</span> jax.random.normal(wkey, shape<span class="op">=</span>(output_size, hidden_sizes[<span class="op">-</span><span class="dv">1</span>])) <span class="op">*</span> jax.numpy.sqrt(<span class="fl">2.</span> <span class="op">/</span> hidden_sizes[<span class="op">-</span><span class="dv">1</span>])</span>
<span id="cb6-31"><a href="#cb6-31" aria-hidden="true" tabindex="-1"></a>  b_out <span class="op">=</span> jax.random.normal(bkey, shape<span class="op">=</span>(output_size,))</span>
<span id="cb6-32"><a href="#cb6-32" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> params, {<span class="st">&#39;W&#39;</span>: W_out, <span class="st">&#39;b&#39;</span>: b_out}</span>
<span id="cb6-33"><a href="#cb6-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-34"><a href="#cb6-34" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> predict(params, xs):</span>
<span id="cb6-35"><a href="#cb6-35" aria-hidden="true" tabindex="-1"></a>  layers, head <span class="op">=</span> params</span>
<span id="cb6-36"><a href="#cb6-36" aria-hidden="true" tabindex="-1"></a>  eps <span class="op">=</span> <span class="fl">1e-5</span></span>
<span id="cb6-37"><a href="#cb6-37" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> l <span class="kw">in</span> layers:</span>
<span id="cb6-38"><a href="#cb6-38" aria-hidden="true" tabindex="-1"></a>    W <span class="op">=</span> l[<span class="st">&#39;W&#39;</span>]</span>
<span id="cb6-39"><a href="#cb6-39" aria-hidden="true" tabindex="-1"></a>    b <span class="op">=</span> l[<span class="st">&#39;b&#39;</span>]</span>
<span id="cb6-40"><a href="#cb6-40" aria-hidden="true" tabindex="-1"></a>    g <span class="op">=</span> l[<span class="st">&#39;g&#39;</span>]</span>
<span id="cb6-41"><a href="#cb6-41" aria-hidden="true" tabindex="-1"></a>    <span class="co"># LayerNorm</span></span>
<span id="cb6-42"><a href="#cb6-42" aria-hidden="true" tabindex="-1"></a>    a <span class="op">=</span> xs <span class="op">@</span> W.T</span>
<span id="cb6-43"><a href="#cb6-43" aria-hidden="true" tabindex="-1"></a>    mu <span class="op">=</span> a.mean(axis<span class="op">=</span><span class="dv">1</span>, keepdims<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb6-44"><a href="#cb6-44" aria-hidden="true" tabindex="-1"></a>    sigma <span class="op">=</span> a.std(axis<span class="op">=</span><span class="dv">1</span>, keepdims<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb6-45"><a href="#cb6-45" aria-hidden="true" tabindex="-1"></a>    a <span class="op">=</span> g <span class="op">*</span> (a <span class="op">-</span> mu) <span class="op">/</span> (eps <span class="op">+</span> sigma)</span>
<span id="cb6-46"><a href="#cb6-46" aria-hidden="true" tabindex="-1"></a>    <span class="co"># ReLU</span></span>
<span id="cb6-47"><a href="#cb6-47" aria-hidden="true" tabindex="-1"></a>    xs <span class="op">=</span> jax.nn.relu(a <span class="op">+</span> b)</span>
<span id="cb6-48"><a href="#cb6-48" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Output: No norm/nonlinearity.</span></span>
<span id="cb6-49"><a href="#cb6-49" aria-hidden="true" tabindex="-1"></a>  W, b <span class="op">=</span> head[<span class="st">&#39;W&#39;</span>], head[<span class="st">&#39;b&#39;</span>]</span>
<span id="cb6-50"><a href="#cb6-50" aria-hidden="true" tabindex="-1"></a>  xs <span class="op">=</span> xs <span class="op">@</span> W.T <span class="op">+</span> b</span>
<span id="cb6-51"><a href="#cb6-51" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> xs</span>
<span id="cb6-52"><a href="#cb6-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-53"><a href="#cb6-53" aria-hidden="true" tabindex="-1"></a><span class="at">@jax.jit</span></span>
<span id="cb6-54"><a href="#cb6-54" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> mse_loss(params, xs, ys):</span>
<span id="cb6-55"><a href="#cb6-55" aria-hidden="true" tabindex="-1"></a>  preds <span class="op">=</span> predict(params, xs)</span>
<span id="cb6-56"><a href="#cb6-56" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> jnp.mean((ys <span class="op">-</span> preds) <span class="op">**</span> <span class="dv">2</span>)</span>
<span id="cb6-57"><a href="#cb6-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-58"><a href="#cb6-58" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> step(opt, params, opt_state, xb, yb):</span>
<span id="cb6-59"><a href="#cb6-59" aria-hidden="true" tabindex="-1"></a>  l, dl <span class="op">=</span> jax.value_and_grad(mse_loss)(params, xb, yb)</span>
<span id="cb6-60"><a href="#cb6-60" aria-hidden="true" tabindex="-1"></a>  <span class="co">#print(dl)</span></span>
<span id="cb6-61"><a href="#cb6-61" aria-hidden="true" tabindex="-1"></a>  updates, opt_state <span class="op">=</span> opt.update(dl, opt_state, params)</span>
<span id="cb6-62"><a href="#cb6-62" aria-hidden="true" tabindex="-1"></a>  params <span class="op">=</span> optax.apply_updates(params, updates)</span>
<span id="cb6-63"><a href="#cb6-63" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> <span class="bu">tuple</span>(params), opt_state</span>
<span id="cb6-64"><a href="#cb6-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-65"><a href="#cb6-65" aria-hidden="true" tabindex="-1"></a>step <span class="op">=</span> jax.jit(step, static_argnames<span class="op">=</span><span class="st">&#39;opt&#39;</span>)</span>
<span id="cb6-66"><a href="#cb6-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-67"><a href="#cb6-67" aria-hidden="true" tabindex="-1"></a><span class="co"># Train/val split</span></span>
<span id="cb6-68"><a href="#cb6-68" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-69"><a href="#cb6-69" aria-hidden="true" tabindex="-1"></a>ntrain <span class="op">=</span> <span class="bu">int</span>(<span class="fl">.8</span> <span class="op">*</span> N)</span>
<span id="cb6-70"><a href="#cb6-70" aria-hidden="true" tabindex="-1"></a>xs_train, ys_train <span class="op">=</span> X[:ntrain], ys[:ntrain]</span>
<span id="cb6-71"><a href="#cb6-71" aria-hidden="true" tabindex="-1"></a>xs_val, ys_val <span class="op">=</span> X[ntrain:], ys[ntrain:]</span>
<span id="cb6-72"><a href="#cb6-72" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&#39;Training set: </span><span class="sc">{</span>xs_train<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">, </span><span class="sc">{</span>ys_train<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">&#39;</span>)</span>
<span id="cb6-73"><a href="#cb6-73" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&#39;Validation set: </span><span class="sc">{</span>xs_val<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">, </span><span class="sc">{</span>ys_val<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">&#39;</span>)</span>
<span id="cb6-74"><a href="#cb6-74" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-75"><a href="#cb6-75" aria-hidden="true" tabindex="-1"></a><span class="co"># Model instantiation</span></span>
<span id="cb6-76"><a href="#cb6-76" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-77"><a href="#cb6-77" aria-hidden="true" tabindex="-1"></a>key, subkey <span class="op">=</span> jax.random.split(key)</span>
<span id="cb6-78"><a href="#cb6-78" aria-hidden="true" tabindex="-1"></a>params <span class="op">=</span> init_mlp(subkey, <span class="dv">5</span>, [<span class="dv">8</span>, <span class="dv">6</span>, <span class="dv">6</span>], <span class="dv">1</span>)</span>
<span id="cb6-79"><a href="#cb6-79" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-80"><a href="#cb6-80" aria-hidden="true" tabindex="-1"></a><span class="co"># Training</span></span>
<span id="cb6-81"><a href="#cb6-81" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-82"><a href="#cb6-82" aria-hidden="true" tabindex="-1"></a>opt <span class="op">=</span> optax.adamw(learning_rate<span class="op">=</span><span class="fl">0.01</span>, weight_decay<span class="op">=</span><span class="fl">0.001</span>)</span>
<span id="cb6-83"><a href="#cb6-83" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-84"><a href="#cb6-84" aria-hidden="true" tabindex="-1"></a>key, subkey <span class="op">=</span> jax.random.split(key)</span>
<span id="cb6-85"><a href="#cb6-85" aria-hidden="true" tabindex="-1"></a>idx <span class="op">=</span> jax.random.permutation(subkey, ntrain)</span>
<span id="cb6-86"><a href="#cb6-86" aria-hidden="true" tabindex="-1"></a>niter <span class="op">=</span> <span class="dv">1000</span></span>
<span id="cb6-87"><a href="#cb6-87" aria-hidden="true" tabindex="-1"></a>nbatch <span class="op">=</span> <span class="dv">256</span></span>
<span id="cb6-88"><a href="#cb6-88" aria-hidden="true" tabindex="-1"></a>start <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb6-89"><a href="#cb6-89" aria-hidden="true" tabindex="-1"></a>opt_state <span class="op">=</span> opt.init(params)</span>
<span id="cb6-90"><a href="#cb6-90" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-91"><a href="#cb6-91" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(niter):</span>
<span id="cb6-92"><a href="#cb6-92" aria-hidden="true" tabindex="-1"></a>  key, subkey <span class="op">=</span> jax.random.split(key)</span>
<span id="cb6-93"><a href="#cb6-93" aria-hidden="true" tabindex="-1"></a>  xb, yb <span class="op">=</span> xs_train[idx[start:start<span class="op">+</span>nbatch]], ys_train[idx[start:start<span class="op">+</span>nbatch]]</span>
<span id="cb6-94"><a href="#cb6-94" aria-hidden="true" tabindex="-1"></a>  start <span class="op">+=</span> nbatch</span>
<span id="cb6-95"><a href="#cb6-95" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> start <span class="op">&gt;=</span> ntrain:</span>
<span id="cb6-96"><a href="#cb6-96" aria-hidden="true" tabindex="-1"></a>    start <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb6-97"><a href="#cb6-97" aria-hidden="true" tabindex="-1"></a>    key, subkey <span class="op">=</span> jax.random.split(key)</span>
<span id="cb6-98"><a href="#cb6-98" aria-hidden="true" tabindex="-1"></a>    idx <span class="op">=</span> jax.random.permutation(subkey, ntrain)</span>
<span id="cb6-99"><a href="#cb6-99" aria-hidden="true" tabindex="-1"></a>  params, opt_state <span class="op">=</span> step(opt, params, opt_state, xb, yb)</span>
<span id="cb6-100"><a href="#cb6-100" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> i <span class="op">%</span> <span class="dv">100</span> <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb6-101"><a href="#cb6-101" aria-hidden="true" tabindex="-1"></a>    l <span class="op">=</span> mse_loss(params, xb, yb)</span>
<span id="cb6-102"><a href="#cb6-102" aria-hidden="true" tabindex="-1"></a>    train_loss <span class="op">=</span> mse_loss(params, xs_train, ys_train)</span>
<span id="cb6-103"><a href="#cb6-103" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f&#39;Iteration </span><span class="sc">{</span>i<span class="sc">}</span><span class="ss">, batch loss = </span><span class="sc">{</span>l<span class="sc">.</span>item()<span class="sc">}</span><span class="ss">, train loss = </span><span class="sc">{</span>train_loss<span class="sc">.</span>item()<span class="sc">}</span><span class="ss">&#39;</span>)</span>
<span id="cb6-104"><a href="#cb6-104" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&#39;Final train loss = </span><span class="sc">{</span>mse_loss(params, xs_train, ys_train)<span class="sc">.</span>item()<span class="sc">}</span><span class="ss">&#39;</span>)</span>
<span id="cb6-105"><a href="#cb6-105" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&#39;Final val loss = </span><span class="sc">{</span>mse_loss(params, xs_val, ys_val)<span class="sc">.</span>item()<span class="sc">}</span><span class="ss">&#39;</span>)</span>
<span id="cb6-106"><a href="#cb6-106" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-107"><a href="#cb6-107" aria-hidden="true" tabindex="-1"></a></span></code></pre></div>
<pre><code>Generated synthetic data: X: (10000, 5), ys: (10000, 1)
Training set: (8000, 5), (8000, 1)
Validation set: (2000, 5), (2000, 1)
Iteration 0, batch loss = 5.191179275512695, train loss = 4.737359523773193
Iteration 100, batch loss = 0.6641169786453247, train loss = 0.7498972415924072
Iteration 200, batch loss = 0.5052511096000671, train loss = 0.6198509335517883
Iteration 300, batch loss = 0.4988126754760742, train loss = 0.5501430034637451
Iteration 400, batch loss = 0.5596280694007874, train loss = 0.49914073944091797
Iteration 500, batch loss = 0.4847404360771179, train loss = 0.5024611353874207
Iteration 600, batch loss = 0.6246224045753479, train loss = 0.48184746503829956
Iteration 700, batch loss = 0.45626115798950195, train loss = 0.47913187742233276
Iteration 800, batch loss = 0.5729255676269531, train loss = 0.4928268492221832
Iteration 900, batch loss = 0.5909319519996643, train loss = 0.4822283983230591
Final train loss = 0.5036829710006714
Final val loss = 0.5718706250190735</code></pre>
</section>
</body>
</html>
