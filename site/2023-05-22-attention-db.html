<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="dcterms.date" content="2023-04-30" />
  <title>Attention - this value does not exist</title>
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
    /* CSS for syntax highlighting */
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #aaaaaa;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
    div.sourceCode
      {   }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span.al { color: #ff0000; font-weight: bold; } /* Alert */
    code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
    code span.at { color: #7d9029; } /* Attribute */
    code span.bn { color: #40a070; } /* BaseN */
    code span.bu { color: #008000; } /* BuiltIn */
    code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
    code span.ch { color: #4070a0; } /* Char */
    code span.cn { color: #880000; } /* Constant */
    code span.co { color: #60a0b0; font-style: italic; } /* Comment */
    code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
    code span.do { color: #ba2121; font-style: italic; } /* Documentation */
    code span.dt { color: #902000; } /* DataType */
    code span.dv { color: #40a070; } /* DecVal */
    code span.er { color: #ff0000; font-weight: bold; } /* Error */
    code span.ex { } /* Extension */
    code span.fl { color: #40a070; } /* Float */
    code span.fu { color: #06287e; } /* Function */
    code span.im { color: #008000; font-weight: bold; } /* Import */
    code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
    code span.kw { color: #007020; font-weight: bold; } /* Keyword */
    code span.op { color: #666666; } /* Operator */
    code span.ot { color: #007020; } /* Other */
    code span.pp { color: #bc7a00; } /* Preprocessor */
    code span.sc { color: #4070a0; } /* SpecialChar */
    code span.ss { color: #bb6688; } /* SpecialString */
    code span.st { color: #4070a0; } /* String */
    code span.va { color: #19177c; } /* Variable */
    code span.vs { color: #4070a0; } /* VerbatimString */
    code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
  </style>
  <link rel="stylesheet" href="style.css" />
  <script>
  document.addEventListener('DOMContentLoaded', function() {
    document.querySelectorAll('.zippy').forEach(zippy => {
      const previewText = zippy.getAttribute('data-preview') || 'Click to expand';
      zippy.innerHTML = `
        <div class="zippy-header-collapsed">${previewText} ▶</div>
        <div class="zippy-header-expanded">${previewText} ▼</div>
        <div class="zippy-content">${zippy.innerHTML}</div>
      `;

      zippy.addEventListener('click', function() {
        this.classList.toggle('expanded');
      });
    });
  });
  </script>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
</head>
<body>
<nav id="mainnav">
  <a href="index.html">Home</a>
</nav>
<header id="title-block-header">
<h1 class="title">Attention - this value does not exist</h1>
<p class="date">2023-04-30</p>
</header>
<p>Here, we play with the attention mechanism and word embeddings to get
make a KV store that fuzzily searches through the key space, and might
<em>also</em> return candidate values that are nonexistent in the
store.</p>
<p>Suppose we have the following key/value pairs:</p>
<pre><code>{
  &#39;one&#39;: &#39;number&#39;,
  &#39;cat&#39;: &#39;animal&#39;,
  &#39;dog&#39;: &#39;animal&#39;,
  &#39;ball&#39;: &#39;toy&#39;,
  &#39;sphere&#39;: &#39;shape&#39;,
  &#39;male&#39;: &#39;gender&#39;,
}</code></pre>
<p>Here are some example queries and their results:</p>
<pre><code>rhombus  [&#39;shape&#39; &#39;shaped&#39; &#39;fit&#39; &#39;fits&#39; &#39;resembles&#39;]
parabola [&#39;shape&#39; &#39;shaped&#39; &#39;fit&#39; &#39;fits&#39; &#39;resembles&#39;]
female   [&#39;gender&#39; &#39;ethnicity&#39; &#39;orientation&#39; &#39;racial&#39; &#39;mainstreaming&#39;]
seven    [&#39;number&#39; &#39;numbers&#39; &#39;ten&#39; &#39;only&#39; &#39;other&#39;]
seventy  [&#39;gender&#39; &#39;ethnicity&#39; &#39;orientation&#39; &#39;regardless&#39; &#39;defining&#39;]
cylinder [&#39;toy&#39; &#39;instance&#39; &#39;unlike&#39; &#39;besides&#39; &#39;newest&#39;]</code></pre>
<p>So as we can see, the top result for <code>seven</code> is
<code>number</code>, because <code>seven</code> is similar to
<code>one</code>. but there are also other results that are not in the
dataset.</p>
<p>Then there is <code>seventy</code>, which should also have returned
<code>number</code> as at least one result, but it didn’t. Somehow our
store thinks <code>seventy</code> is closer to <code>male</code> than to
<code>one</code>.</p>
<h2 id="answering-queries-with-attention">Answering queries with
attention</h2>
<h3 id="the-interface">The interface</h3>
<div class="sourceCode" id="cb3"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Init with a dict containing they key/value pairs.</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>db <span class="op">=</span> Database({</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>  <span class="st">&#39;key1&#39;</span>: <span class="st">&#39;val1&#39;</span>,</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>  <span class="st">&#39;key2&#39;</span>: <span class="st">&#39;val2,</span></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="er">})</span></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a><span class="er"># Query</span></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>results <span class="op">=</span> db.query(<span class="st">&#39;foo&#39;</span>)</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a><span class="cf">assert</span> <span class="bu">type</span>(results) <span class="op">==</span> <span class="bu">list</span></span></code></pre></div>
<h3 id="answering-queries-with-attention-1">Answering queries with
attention</h3>
<p>Consider a dataset D containing key-value pairs <span
class="math inline">\(D = \{(\textbf{k}_1, \textbf{v}_1), ...,
(\textbf{k}_m, \textbf{v}_m)\}\)</span>.</p>
<p>Imagine a query mechanism that, given a query <span
class="math inline">\(\textbf{q}\)</span>, produces a result given
by:</p>
<p><span class="math display">\[
Attention(\textbf{q}, D) = \sum_{i=1}^{m}\alpha(\textbf{q},
\textbf{k}_i) \textbf{v}_i
\]</span></p>
<p>Note:</p>
<ul>
<li>The weights <span class="math inline">\(\alpha(\textbf{q},
\textbf{k}_i)\)</span> are non-negative, and sum to 1.</li>
<li>We can make this an exact key match retrieval by setting <span
class="math inline">\(\alpha(\textbf{q}, \textbf{k}_i)=1\)</span> when
<span class="math inline">\(q=\textbf{k}_i\)</span>, and <span
class="math inline">\(0\)</span> otherwise.</li>
<li>We can get average pooling OTOH if we set <span
class="math inline">\(\alpha(\textbf{q},
\textbf{k}_i)=\frac{1}{m}\)</span>.</li>
</ul>
<h3 id="implementation">Implementation</h3>
<p>Our DB interface uses variable length words, but the attention
mechanism wants fixed-length vectors. While we could use a simple
one-hot encoding of words, it won’t capture the semantic closeness of
words.</p>
<p>Instead, we’ll use the <a
href="https://nlp.stanford.edu/projects/glove/">GloVe</a> embeddings,
that allow us to represent variable length words with their fixed size
embeddings.</p>
<p>Sketch:</p>
<ul>
<li>For each k, v in the records dict, create embeddings for k and v and
put them in separate tensors, called keys and values.
<ul>
<li>keys and values both have a shape of
<code>(len(records), embed_size)</code></li>
</ul></li>
<li>Given a query,
<ul>
<li>Create its embedding into a vector <code>q_embed</code> of shape
<code>(embed_size,)</code></li>
<li>compute <code>w = softmax(keys.mul(q_embed))</code> to get a
<code>(len(records),)</code> shaped vector of attention weights.</li>
<li>Compute the attention as <code>sum(w .* values)</code> to get a
weighted sum of values in a <code>(n_embed,)</code> vector.</li>
<li>Reverse the weighted-sum vector thus obtained into candidate words
by searching the embedding space for nearby words.</li>
</ul></li>
</ul>
<p>Runnable code in
https://colab.research.google.com/drive/1ReE-WIEzAGNr1a-6TUHsQucwtgWFhuTF?usp=sharing</p>
</body>
</html>
